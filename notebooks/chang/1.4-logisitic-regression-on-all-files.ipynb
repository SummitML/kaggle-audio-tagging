{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "I want to use two out of the box features: chroma_stft and mfcc from librosa. For each feature, I will make a simple logistic regression model, then combine the output from both models as new features to train another model on top of it. \n",
    "\n",
    "Several friends had commented in the past that stacked/ensemble models tend to work better than a single model, so I want to try the workflow here.\n",
    "\n",
    "![stacked](../../docs/images/chang-1.4-stacked-model.png)\n",
    "\n",
    "Also got the idea off Ch.50 page 3 of Andrew Ng's book Deep Learning Yearning he's sending out in emails\n",
    "\n",
    "The results are evaluated by a 75%-25% train-test split with the out of the box metric (mean accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import librosa.feature as lf\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data for later use\n",
    "\n",
    "**Can skip reading this whole section** - basically load and process it using Carlos's function to either read the whole audio or only the first second.\n",
    "\n",
    "Somehow I always have memory error in reading Applauses when I try to parse the whole thing. It may have to do with the 32-bit Python installation I have on Windows 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'data/external/train.csv'\n",
    "TRAIN_FILES = 'data/external/audio_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(pd.read_csv(TRAIN_CSV).label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each label and process into 1 second clips separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0-Hi-hat\n",
      "Processed 1-Saxophone\n",
      "Processed 2-Trumpet\n",
      "Processed 3-Glockenspiel\n",
      "Processed 4-Cello\n",
      "Processed 5-Knock\n",
      "Processed 6-Gunshot_or_gunfire\n",
      "Processed 7-Clarinet\n",
      "Processed 8-Computer_keyboard\n",
      "Processed 9-Keys_jangling\n",
      "Processed 10-Snare_drum\n",
      "Processed 11-Writing\n",
      "Processed 12-Laughter\n",
      "Processed 13-Tearing\n",
      "Processed 14-Fart\n",
      "Processed 15-Oboe\n",
      "Processed 16-Flute\n",
      "Processed 17-Cough\n",
      "Processed 18-Telephone\n",
      "Processed 19-Bark\n",
      "Processed 20-Chime\n",
      "Processed 21-Bass_drum\n",
      "Processed 22-Bus\n",
      "Processed 23-Squeak\n",
      "Processed 24-Scissors\n",
      "Processed 25-Harmonica\n",
      "Processed 26-Gong\n",
      "Processed 27-Microwave_oven\n",
      "Processed 28-Burping_or_eructation\n",
      "Processed 29-Double_bass\n",
      "Processed 30-Shatter\n",
      "Processed 31-Fireworks\n",
      "Processed 32-Tambourine\n",
      "Processed 33-Cowbell\n",
      "Processed 34-Electric_piano\n",
      "Processed 35-Meow\n",
      "Processed 36-Drawer_open_or_close\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f74447e0ee90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mwav_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_wav_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/raw/train/wav-data-{}.pkl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processed {}-{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for tag_num, tag_name in enumerate(tags):\n",
    "    train_files = helpers.find_paths_with_tags(csv_path=TRAIN_CSV, files_path=TRAIN_FILES, filters=[tag_name])\n",
    "    wav_data = helpers.load_wav_files(train_files, duration=15)\n",
    "    with open('data/raw/train/wav-data-{}.pkl'.format(tag_name), 'wb') as f:\n",
    "        pickle.dump(wav_data, f)\n",
    "    print('Processed {}-{}'.format(tag_num, tag_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0-Applause\n",
      "Processed 1-Acoustic_guitar\n",
      "Processed 2-Violin_or_fiddle\n",
      "Processed 3-Finger_snapping\n"
     ]
    }
   ],
   "source": [
    "for tag_num, tag_name in enumerate(tags[37:]):\n",
    "    train_files = helpers.find_paths_with_tags(csv_path=TRAIN_CSV, files_path=TRAIN_FILES, filters=[tag_name])\n",
    "    wav_data = helpers.load_wav_files(train_files, duration=15)\n",
    "    with open('data/raw/train/wav-data-{}.pkl'.format(tag_name), 'wb') as f:\n",
    "        pickle.dump(wav_data, f)\n",
    "    print('Processed {}-{}'.format(tag_num, tag_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 second versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0-Hi-hat\n",
      "Processed 1-Saxophone\n",
      "Processed 2-Trumpet\n",
      "Processed 3-Glockenspiel\n",
      "Processed 4-Cello\n",
      "Processed 5-Knock\n",
      "Processed 6-Gunshot_or_gunfire\n",
      "Processed 7-Clarinet\n",
      "Processed 8-Computer_keyboard\n",
      "Processed 9-Keys_jangling\n",
      "Processed 10-Snare_drum\n",
      "Processed 11-Writing\n",
      "Processed 12-Laughter\n",
      "Processed 13-Tearing\n",
      "Processed 14-Fart\n",
      "Processed 15-Oboe\n",
      "Processed 16-Flute\n",
      "Processed 17-Cough\n",
      "Processed 18-Telephone\n",
      "Processed 19-Bark\n",
      "Processed 20-Chime\n",
      "Processed 21-Bass_drum\n",
      "Processed 22-Bus\n",
      "Processed 23-Squeak\n",
      "Processed 24-Scissors\n",
      "Processed 25-Harmonica\n",
      "Processed 26-Gong\n",
      "Processed 27-Microwave_oven\n",
      "Processed 28-Burping_or_eructation\n",
      "Processed 29-Double_bass\n",
      "Processed 30-Shatter\n",
      "Processed 31-Fireworks\n",
      "Processed 32-Tambourine\n",
      "Processed 33-Cowbell\n",
      "Processed 34-Electric_piano\n",
      "Processed 35-Meow\n",
      "Processed 36-Drawer_open_or_close\n",
      "Processed 37-Applause\n",
      "Processed 38-Acoustic_guitar\n",
      "Processed 39-Violin_or_fiddle\n",
      "Processed 40-Finger_snapping\n"
     ]
    }
   ],
   "source": [
    "for tag_num, tag_name in enumerate(tags):\n",
    "    train_files = helpers.find_paths_with_tags(csv_path=TRAIN_CSV, files_path=TRAIN_FILES, filters=[tag_name])\n",
    "    wav_data = helpers.load_wav_files(train_files, duration=1)\n",
    "    with open('data/raw/train-1-sec/wav-data-{}.pkl'.format(tag_name), 'wb') as f:\n",
    "        pickle.dump(wav_data, f)\n",
    "    print('Processed {}-{}'.format(tag_num, tag_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I processed the files to extract their features.\n",
    "\n",
    "If an audio is less than 1 second long, then pad it to 1 second before processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(sound: np.ndarray, sample_rate=22050):\n",
    "    padded_sound = np.tile(sound, math.ceil(sample_rate / sound.shape[0]))\n",
    "    return padded_sound[:sample_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chroma_stft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each label, read in the 1 second version, process with librosa.features.chroma_stft, then save it back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chang\\.virtualenvs\\kaggle-audio-tagging-pu_5w54x\\lib\\site-packages\\librosa\\core\\pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_name = 'chroma_stft'\n",
    "for tag_num, tag_name in enumerate(tags):\n",
    "    with open('data/raw/train-1-sec/wav-data-{}.pkl'.format(tag_name), 'rb') as f:\n",
    "        wav_data = pickle.load(f)\n",
    "    wav_features = {sample.name: lf.chroma_stft(pad_audio(sample.wav[0])).flatten() for sample in wav_data}\n",
    "    df_features = (\n",
    "        pd.DataFrame.from_dict(wav_features, orient='index')\n",
    "        .reset_index().rename({'index': 'name'}, axis=1)\n",
    "    )\n",
    "    df_features.columns = ['name'] + [feature_name + '_' + str(column_name) for column_name in list(df_features.columns)[1:]]\n",
    "    df_features.to_pickle('data/interim/{}-1-sec/{}.pkl'.format(feature_name, tag_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each label, read in the 1 second version, process with librosa.features.mfcc, then save it back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_name = 'mfcc'\n",
    "for tag_num, tag_name in enumerate(tags):\n",
    "    with open('data/raw/train-1-sec/wav-data-{}.pkl'.format(tag_name), 'rb') as f:\n",
    "        wav_data = pickle.load(f)\n",
    "    wav_features = {sample.name: lf.mfcc(pad_audio(sample.wav[0])).flatten() for sample in wav_data}\n",
    "    df_features = (\n",
    "        pd.DataFrame.from_dict(wav_features, orient='index')\n",
    "        .reset_index().rename({'index': 'name'}, axis=1)\n",
    "    )\n",
    "    df_features.columns = ['name'] + [feature_name + '_' + str(column_name) for column_name in list(df_features.columns)[1:]]\n",
    "    df_features.to_pickle('data/interim/{}-1-sec/{}.pkl'.format(feature_name, tag_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make two models\n",
    "\n",
    "One for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the files \n",
    "\n",
    "Because scikit learn takes in numeric class labels only, I need to convert the string labels to numbers. \n",
    "\n",
    "EDIT: Only after I did it I found out about [sklearn.preprocessing.LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), so I will use that instead in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_reference = pd.Series(tags).rename('label').to_frame().reset_index().rename({'index': 'label_index'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_labels = (\n",
    "    pd.read_csv(TRAIN_CSV)\n",
    "    .drop(['manually_verified'], axis=1)\n",
    "    .rename({'fname': 'name'}, axis=1)\n",
    "    .merge(label_reference, on=['label'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test set\n",
    "\n",
    "For both models, I need to split it into a training set and a test set so I can score later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chroma_stft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all features from processed files\n",
    "train_chroma_stft = pd.concat([pd.read_pickle('data/interim/{}-1-sec/{}.pkl'.format('chroma_stft', name)) for name in tags])\n",
    "\n",
    "df_train_chroma_stft = (\n",
    "    train_chroma_stft\n",
    "    .merge(file_labels, on=['name'], how='inner')\n",
    ")\n",
    "\n",
    "X_chroma_train, X_chroma_test, y_chroma_train, y_chroma_test = model_selection.train_test_split(\n",
    "    df_train_chroma_stft.drop(['name', 'label', 'label_index'], axis=1).values,\n",
    "    df_train_chroma_stft['label_index'].values,\n",
    "    test_size=0.25, \n",
    "    random_state=707,\n",
    "    stratify=df_train_chroma_stft['label_index'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the multiclass version of logistic regression, don't balance class weight yet\n",
    "\n",
    "##### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_model = linear_model.LogisticRegression(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=123, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chroma_model.fit(X_chroma_train, y_chroma_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/interim/{}-1-sec/{}.pkl'.format('chroma_stft', 'logreg-model'), 'wb') as f:\n",
    "    pickle.dump(chroma_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scoring\n",
    "\n",
    "Uses the mean class accuracy. See [This link](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score) on the `.score` method for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16673701983959477"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_model.score(X_chroma_test, y_chroma_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all features from processed files\n",
    "train_mfcc = pd.concat([pd.read_pickle('data/interim/{}-1-sec/{}.pkl'.format('mfcc', name)) for name in tags])\n",
    "\n",
    "df_train_mfcc = (\n",
    "    train_mfcc\n",
    "    .merge(file_labels, on=['name'], how='inner')\n",
    ")\n",
    "\n",
    "X_mfcc_train, X_mfcc_test, y_mfcc_train, y_mfcc_test = model_selection.train_test_split(\n",
    "    df_train_mfcc.drop(['name', 'label', 'label_index'], axis=1).values,\n",
    "    df_train_mfcc['label_index'].values,\n",
    "    test_size=0.25, \n",
    "    random_state=707,\n",
    "    stratify=df_train_mfcc['label_index'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the multiclass version of logistic regression. I changed to tolerance to 0.001 from 0.0001 because the latter was taking forever (> 30 minutes) to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_model = linear_model.LogisticRegression(tol=0.001, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=123, solver='liblinear', tol=0.001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mfcc_model.fit(X_mfcc_train, y_mfcc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/interim/{}-1-sec/{}.pkl'.format('mfcc', 'logreg-model'), 'wb') as f:\n",
    "    pickle.dump(mfcc_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scoring\n",
    "\n",
    "See [This link](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score) on the `.score` method for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28070915998311524"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_model.score(X_mfcc_test, y_mfcc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack the two models up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the output from model 1 and 2 into probability predictions.\n",
    "\n",
    "I used log(probability) instead of raw class probability for no particular reason. \n",
    "\n",
    "##### read the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/interim/{}-1-sec/{}.pkl'.format('chroma_stft', 'logreg-model'), 'rb') as f:\n",
    "    chroma_model = pickle.load(f)\n",
    "\n",
    "with open('data/interim/{}-1-sec/{}.pkl'.format('mfcc', 'logreg-model'), 'rb') as f:\n",
    "    mfcc_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that y_chroma_train and y_mfcc_train are the same - actually I only need to split it once in the preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(y_chroma_train, y_mfcc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use predictions from both models as new features\n",
    "\n",
    "Because there are 41 classes, each model gives me 41 features. Taking both will give me 41 + 41 = 82 features to train the stacked model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature1_train = chroma_model.predict_log_proba(X_chroma_train)\n",
    "X_feature1_test = chroma_model.predict_log_proba(X_chroma_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature2_train = mfcc_model.predict_log_proba(X_mfcc_train)\n",
    "X_feature2_test = mfcc_model.predict_log_proba(X_mfcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7104, 41)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7104, 41)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_feature1_train.shape)\n",
    "display(X_feature2_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the outputs from model 1 and 2\n",
    "\n",
    "Combine the features by axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7104, 82)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = np.concatenate([X_feature1_train, X_feature2_train], axis=1)\n",
    "X_test = np.concatenate([X_feature1_test, X_feature2_test], axis=1)\n",
    "display(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model = linear_model.LogisticRegression(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=123, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stacked_model.fit(X_train, y_mfcc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30434782608695654"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model.score(X_test, y_mfcc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance with mean accuracy\n",
    "\n",
    "* only chroma_stft features: 16.6%\n",
    "* only mfcc features: 28.0%\n",
    "* Stacked logistic regression model using output from chroma_stft and mfcc: 30.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
